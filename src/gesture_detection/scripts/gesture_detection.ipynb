{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 21:18:47.412265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 21:18:47.588671: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-12 21:18:48.235917: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/pumid/Desktop/athome/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda-11.2/lib64:/home/pumid/ros2_ws/install/realsense2_camera/lib:/home/pumid/ros2_ws/install/realsense2_camera_msgs/lib:/home/pumid/Desktop/fra333/lab3_ws/install/xicro_interfaces/lib:/home/pumid/Desktop/fra333/lab2_ws/install/scarlet_kinematics_interfaces/lib:/home/pumid/Desktop/fra333/FRA333-week3/install/my_first_package/lib:/home/pumid/Desktop/fra333/fra333_lab1_32/install/lab1_interfaces/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib:/usr/share/gazebo/../../lib/x86_64-linux-gnu/gazebo-11/plugins:\n",
      "2022-12-12 21:18:48.235996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/pumid/Desktop/athome/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda-11.2/lib64:/home/pumid/ros2_ws/install/realsense2_camera/lib:/home/pumid/ros2_ws/install/realsense2_camera_msgs/lib:/home/pumid/Desktop/fra333/lab3_ws/install/xicro_interfaces/lib:/home/pumid/Desktop/fra333/lab2_ws/install/scarlet_kinematics_interfaces/lib:/home/pumid/Desktop/fra333/FRA333-week3/install/my_first_package/lib:/home/pumid/Desktop/fra333/fra333_lab1_32/install/lab1_interfaces/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib:/usr/share/gazebo/../../lib/x86_64-linux-gnu/gazebo-11/plugins:\n",
      "2022-12-12 21:18:48.236002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This module is in /home/pumid/Desktop/athome/athome_ws/src/gesture_detection/scripts/model/keypoint_classifier\n",
      "Your are using Tensorflow library\n",
      "/home/pumid/Desktop/athome/athome_ws/src/gesture_detection/scripts\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "# from model.keypoint_classifier.coral_keypoint_classifier import CoralKeyPointClassifier\n",
    "# from model import PointHistoryClassifier\n",
    "from model.keypoint_classifier.keypoint_classifier import KeyPointClassifier\n",
    "from utils import CvFpsCalc\n",
    "import copy\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "print(os.getcwd())\n",
    "\n",
    "import csv\n",
    "\n",
    "model_path='model/keypoint_classifier/keypoint_classifier.tflite'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/pumid/Desktop/athome/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from pycoral.adapters import classify\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.dataset import read_label_file\n",
    "from pycoral.utils.edgetpu import make_interpreter, list_edge_tpus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "class CoralKeyPointClassifier(object):\n",
    "    def __init__(self, model_path='src/gesture_detection/scripts/model/keypoint_classifier/keypoint_classifier.tflite'):\n",
    "        self.interpreter = make_interpreter(model_path_or_content=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        landmark_list,\n",
    "    ):\n",
    "        input_details_tensor_index = self.input_details[0]['index']\n",
    "        self.interpreter.set_tensor(\n",
    "            input_details_tensor_index,\n",
    "            np.array([landmark_list], dtype=np.float32))\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        output_details_tensor_index = self.output_details[0]['index']\n",
    "\n",
    "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
    "\n",
    "        result_index = np.argmax(np.squeeze(result))\n",
    "\n",
    "        return result_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### argument #####\n",
    "cap_device = 0\n",
    "cap_width = 960\n",
    "cap_height = 540\n",
    "use_static_image_mode = False\n",
    "min_detection_confidence = 0.7\n",
    "min_tracking_confidence = 0.5\n",
    "\n",
    "##### setting camera #####\n",
    "cap = cv.VideoCapture()\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "\n",
    "##### mediapipe hand #####\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "        static_image_mode=use_static_image_mode,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=min_detection_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence,\n",
    "    )\n",
    "\n",
    "##### load model #####\n",
    "keypoint_classifier = KeyPointClassifier()\n",
    "keypoint_classifier_labels = ['Open', 'Close', 'Pointer', 'OK']\n",
    "# point_history_classifier = PointHistoryClassifier()\n",
    "# point_history_classifier_labels = ['Stop', 'Clockwise', 'Counter Clockwise', 'Move']\n",
    "\n",
    "\n",
    "##### measure FPS #####\n",
    "cvFpsCalc = CvFpsCalc(buffer_len=10)\n",
    "\n",
    "##### declare history point\n",
    "history_length = 16\n",
    "point_history = deque(maxlen=history_length)\n",
    "\n",
    "# Finger gesture history ################################################\n",
    "finger_gesture_history = deque(maxlen=history_length)\n",
    "\n",
    "##### measure fps\n",
    "fps_que = deque(maxlen=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_point_history(image, point_history):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    temp_point_history = copy.deepcopy(point_history)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, point in enumerate(temp_point_history):\n",
    "        if index == 0:\n",
    "            base_x, base_y = point[0], point[1]\n",
    "\n",
    "        temp_point_history[index][0] = (temp_point_history[index][0] - base_x) / image_width\n",
    "        temp_point_history[index][1] = (temp_point_history[index][1] - base_y) / image_height\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_point_history = list(\n",
    "        itertools.chain.from_iterable(temp_point_history))\n",
    "\n",
    "    return temp_point_history\n",
    "    \n",
    "def draw_point_history(image, point_history):\n",
    "    for index, point in enumerate(point_history):\n",
    "        if point[0] != 0 and point[1] != 0:\n",
    "            cv.circle(image, (point[0], point[1]), 1 + int(index / 2),\n",
    "                      (152, 251, 152), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n",
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList'>\n"
     ]
    }
   ],
   "source": [
    "##### setting camera #####\n",
    "cap = cv.VideoCapture(cap_device)\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "while cap.isOpened():\n",
    "    fps = cvFpsCalc.get()\n",
    "    fps_que.append(fps)\n",
    "\n",
    "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    image = cv.flip(image, 1)\n",
    "    debug_image = copy.deepcopy(image)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            ##### test normalize \n",
    "            landmark_point = []\n",
    "            for _, landmark in enumerate(hand_landmarks.landmark):\n",
    "                landmark_point.append([landmark.x, landmark.y, landmark.z])\n",
    "            ##### mediapipe output normarlize convert to pixel\n",
    "            landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "            ##### hand classifier\n",
    "            pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "            print(len(pre_processed_landmark_list))\n",
    "\n",
    "            hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "            # print(hand_sign_id)\n",
    "\n",
    "            # ##### store hand classifier\n",
    "            # if hand_sign_id == 2:  # Point gesture\n",
    "            #     point_history.append(landmark_list[8])\n",
    "            # else:\n",
    "            #     point_history.append([0, 0])\n",
    "\n",
    "            # ##### point history classifier\n",
    "            # finger_gesture_id = 0\n",
    "            # if len(point_history) == history_length:\n",
    "            #     pre_processed_point_history_list = pre_process_point_history(debug_image, point_history)\n",
    "            #     finger_gesture_id = point_history_classifier(pre_processed_point_history_list)\n",
    "\n",
    "            # # Calculates the gesture IDs in the latest detection\n",
    "            # finger_gesture_history.append(finger_gesture_id)\n",
    "            # most_common_fg_id = Counter(finger_gesture_history).most_common()\n",
    "            # dynamic_sign = point_history_classifier_labels[most_common_fg_id[0][0]]\n",
    "\n",
    "\n",
    "            ##### display information|\n",
    "            ### Draw Landmarks\n",
    "            mp_drawing.draw_landmarks(debug_image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                      mp_drawing_styles.get_default_hand_landmarks_style(), mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            ### draw bounding box \n",
    "            brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "            cv.rectangle(debug_image, (brect[0], brect[1]), (brect[2], brect[3]), (0, 0, 0), 1)\n",
    "\n",
    "            ### draw info text\n",
    "            cv.rectangle(debug_image, (brect[0], brect[1]), (brect[2], brect[1] - 22),(0, 0, 0), -1)\n",
    "            hand_sign_text = keypoint_classifier_labels[hand_sign_id]\n",
    "            info_text = handedness.classification[0].label[0:]\n",
    "            if hand_sign_text != \"\":\n",
    "                info_text = info_text + ':' + hand_sign_text\n",
    "            cv.putText(debug_image, info_text, (brect[0] + 5, brect[1] - 4), cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv.LINE_AA)\n",
    "            # finger_gesture_text = dynamic_sign\n",
    "            # if finger_gesture_text != \"\":\n",
    "            #     cv.putText(debug_image, \"Finger Gesture:\" + finger_gesture_text, (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv.LINE_AA)\n",
    "            #     cv.putText(debug_image, \"Finger Gesture:\" + finger_gesture_text, (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2, cv.LINE_AA)\n",
    "\n",
    "                \n",
    "        \n",
    "    debug_image = draw_point_history(debug_image, point_history)\n",
    "    cv.putText(debug_image, \"FPS:\" + str(fps), (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv.LINE_AA)\n",
    "    cv.putText(debug_image, \"FPS:\" + str(fps), (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2, cv.LINE_AA)\n",
    "\n",
    "    cv.imshow('Raw Webcam Feed', debug_image)        \n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'model/keypoint_classifier/test_keypoint.csv'\n",
    "##### setting camera #####\n",
    "cap = cv.VideoCapture(cap_device)\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "while cap.isOpened():\n",
    "    fps = cvFpsCalc.get()\n",
    "\n",
    "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    image = cv.flip(image, 1)\n",
    "    debug_image = copy.deepcopy(image)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            ##### mediapipe output normarlize convert to pixel\n",
    "            landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "            ##### hand classifier\n",
    "            pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "            with open(csv_path, 'a', newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([1, *pre_processed_landmark_list])\n",
    "\n",
    "            ##### display information|\n",
    "            ### Draw Landmarks\n",
    "            mp_drawing.draw_landmarks(debug_image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                      mp_drawing_styles.get_default_hand_landmarks_style(), mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    cv.imshow('Raw Webcam Feed', debug_image)        \n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([1, 1, 2, 3, 1, 1, 1, 0])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('athome')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b67baa72fc1c84083d0ff48b22fcea4e2cd70c93b18c793309c5d605d5c26f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>People Perception &#8212; CACAO@HOME Robot  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/tabs.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Speech Perception" href="speech_perception.html" />
    <link rel="prev" title="Object Perception" href="object_perception.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="people-perception">
<span id="id1"></span><h1>People Perception<a class="headerlink" href="#people-perception" title="Permalink to this heading">¶</a></h1>
<h1 align="center">
  <div>
    <div style="position: relative; padding-bottom: 0%; overflow: hidden; max-width: 100%; height: auto;">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/4Jl3G3RK47c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>
  </div>
</h1><section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>Package for face recognition and people perception for Cacao mobile robot paticipate in ROBOCUP use with navigation to do people following task</p>
<section id="architecture">
<h3>Architecture<a class="headerlink" href="#architecture" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="../../_images/people_detection_arch.png"><img alt="people_detection_arch" class="align-center" src="../../_images/people_detection_arch.png" style="width: 480px;" /></a>
<ul>
<li><p><strong>people_detection_scripts.py</strong></p>
<blockquote>
<div><p>Tracking people and send their position to base_controller.py</p>
</div></blockquote>
</li>
</ul>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"></div><div class="tab-set docutils container">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">Topic</label><div class="tab-content docutils container">
<p>/goal_update[geometry_msgs/PoseStamped]</p>
<blockquote>
<div><p>Publish people position in xy coordinate relative to map to bt naigator for following dynamic object</p>
</div></blockquote>
<p>/people_detection/status[std_msgs/Uint8]</p>
<blockquote>
<div><p>Publish status of the node (0=wait for command,1=running,2=succeed,-1=fail)</p>
</div></blockquote>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">Subscribe</label><div class="tab-content docutils container">
<p>/depth_camera/image_raw[sensor_msgs/Image]</p>
<blockquote>
<div><p>Subscribe RGB image from D455</p>
</div></blockquote>
<p>/depth_camera/depth/image_raw[sensor_msgs/Image]</p>
<blockquote>
<div><p>Subscribe depth image from D455</p>
</div></blockquote>
<p>/depth_camera/depth/camera_info[sensor_msgs/CameraInfo]</p>
<blockquote>
<div><p>Subscribe camera info from D455</p>
</div></blockquote>
</div>
<input class="tab-input" id="tab-set--0-input--3" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--3">Service</label><div class="tab-content docutils container">
<p>/people_detection/enable[std_srvs/Empty]</p>
<blockquote>
<div><p>Call when you want to enable publish tf tranfrom people position to rviz2</p>
</div></blockquote>
<p>/people_detection/arrival[std_srvs/Empty]</p>
<blockquote>
<div><p>Call when you want to tell the robot that you arrive at destination and stop tracking</p>
</div></blockquote>
</div>
</div>
</div>
<ul>
<li><p><strong>base_controller.py</strong></p>
<blockquote>
<div><p>Recieve position from people_detection_scripts.py and compute wheel effort to control actuator for people following</p>
</div></blockquote>
</li>
</ul>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"></div><div class="tab-set docutils container">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">Topic</label><div class="tab-content docutils container">
<p>/cmd_vel[geometry_msgs/Twist]</p>
<blockquote>
<div><p>Publish publist linear velocity and angular velocity of robot to wheel controller node</p>
</div></blockquote>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">Subscribe</label><div class="tab-content docutils container">
<p>/goal_update[geometry_msgs/PoseStamped]</p>
<blockquote>
<div><p>Subscribe tracking position</p>
</div></blockquote>
<p>/people_detection/status[std_msgs/Uint8]</p>
<blockquote>
<div><p>Subscribe status of people follower status(status 1 = follow otherwise stop)</p>
</div></blockquote>
</div>
</div>
</div>
</section>
<section id="concept-how-does-it-work">
<h3>Concept(How does it work?)<a class="headerlink" href="#concept-how-does-it-work" title="Permalink to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>def imageCallback<span class="o">(</span>self, data<span class="o">)</span>:
<span class="nv">cv_image</span> <span class="o">=</span> self.br.imgmsg_to_cv2<span class="o">(</span>data<span class="o">)</span>
self.h, self.w <span class="o">=</span> cv_image.shape<span class="o">[</span>:2<span class="o">]</span>
<span class="c1"># To improve performance, optionally mark the image as not writeable to</span>
<span class="c1"># pass by reference.</span>
cv_image.flags.writeable <span class="o">=</span> False
<span class="nv">cv_image</span> <span class="o">=</span> cv2.cvtColor<span class="o">(</span>cv_image, cv2.COLOR_BGR2RGB<span class="o">)</span>
<span class="nv">results</span> <span class="o">=</span> self.pose.process<span class="o">(</span>cv_image<span class="o">)</span>

<span class="c1"># Draw the pose annotation on the image.</span>
cv_image.flags.writeable <span class="o">=</span> True
<span class="nv">cv_image</span> <span class="o">=</span> cv2.cvtColor<span class="o">(</span>cv_image, cv2.COLOR_RGB2BGR<span class="o">)</span>
self.detect_people <span class="o">=</span> False
<span class="c1"># Flip the image horizontally for a selfie-view display.</span>
<span class="k">if</span> results.pose_landmarks!<span class="o">=</span>None:
    <span class="nv">mid_x_shoulder</span> <span class="o">=</span> <span class="o">(</span>results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.LEFT_SHOULDER<span class="o">]</span>.x +           results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.RIGHT_SHOULDER<span class="o">]</span>.x<span class="o">)</span>/2 * self.w
    <span class="nv">mid_y_shoulder</span> <span class="o">=</span> <span class="o">(</span>results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.LEFT_SHOULDER<span class="o">]</span>.y + results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.RIGHT_SHOULDER<span class="o">]</span>.y<span class="o">)</span>/2 * self.h
    <span class="nv">mid_x_hip</span> <span class="o">=</span> <span class="o">(</span>results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.LEFT_HIP<span class="o">]</span>.x + results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.RIGHT_HIP<span class="o">]</span>.x<span class="o">)</span>/2 * self.w
    <span class="nv">mid_y_hip</span> <span class="o">=</span> <span class="o">(</span>results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.LEFT_HIP<span class="o">]</span>.y + results.pose_landmarks.landmark<span class="o">[</span>self.lmPose.RIGHT_HIP<span class="o">]</span>.y<span class="o">)</span>/2 * self.h
    self.point_x <span class="o">=</span> int<span class="o">((</span>mid_x_shoulder + mid_x_hip<span class="o">)</span>/2<span class="o">)</span>
    <span class="k">if</span> self.point_x &gt; self.w-1:
        self.point_x <span class="o">=</span> self.w-1
    <span class="k">elif</span> self.point_x &lt; <span class="m">0</span>:
        self.point_x <span class="o">=</span> <span class="m">0</span>
    self.point_y <span class="o">=</span> int<span class="o">((</span>mid_y_shoulder + mid_y_hip<span class="o">)</span>/2<span class="o">)</span>
    <span class="k">if</span> self.point_y &gt; self.h-1:
        self.point_y <span class="o">=</span> self.h-1
    <span class="k">elif</span> self.point_y &lt; <span class="m">0</span>:
        self.point_y <span class="o">=</span> <span class="m">0</span>
    cv2.circle<span class="o">(</span>cv_image, <span class="o">(</span>self.point_x,self.point_y<span class="o">)</span>, <span class="m">0</span>, <span class="o">(</span><span class="m">0</span>,0,255<span class="o">)</span>, <span class="m">20</span><span class="o">)</span>
    self.detect_people <span class="o">=</span> True
cv2.imshow<span class="o">(</span><span class="s1">&#39;MediaPipe Pose&#39;</span>, cv2.flip<span class="o">(</span>cv_image, <span class="m">1</span><span class="o">))</span>
cv2.waitKey<span class="o">(</span><span class="m">1</span><span class="o">)</span>
</pre></div>
</div>
<p>in imageCallback we use mediapipe to track center of the human body and get 2D coordinate on camera plane.The minimum body part that need to appear in front of cemra is half the body.Then when node recieve enable service it will save 2D coordinate and wait for imageDepthCallback method callback tu publish 3D coordinate relative to base frame of robot</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>def imageDepthCallback<span class="o">(</span>self, data<span class="o">)</span>:
try:
    <span class="c1">#get image from msg</span>
    <span class="nv">depth_image</span> <span class="o">=</span> self.bridge.imgmsg_to_cv2<span class="o">(</span>data, data.encoding<span class="o">)</span>
    <span class="o">[</span>depth_y, depth_x<span class="o">]</span> <span class="o">=</span> depth_image.shape<span class="o">[</span>:2<span class="o">]</span>
    <span class="c1">#mediapipe pose</span>
    <span class="c1"># self.get_logger().info(f&quot;{depth_image.shape}&quot;)</span>
    <span class="k">if</span> self.intrinsics:
        <span class="k">if</span> self.detect_people and self.follow_enb:
            <span class="c1"># self.get_logger().info(&quot;publish coordinate&quot;)</span>
            <span class="nv">x_depth</span> <span class="o">=</span> int<span class="o">(</span>self.point_x * depth_x / self.w<span class="o">)</span>
            <span class="nv">y_depth</span> <span class="o">=</span> int<span class="o">(</span>self.point_y * depth_y / self.h<span class="o">)</span>
            <span class="nv">depth</span> <span class="o">=</span> depth_image<span class="o">[</span>y_depth, x_depth<span class="o">]</span>
            <span class="nv">XYZ</span> <span class="o">=</span> rs2.rs2_deproject_pixel_to_point<span class="o">(</span>self.intrinsics, <span class="o">[</span>x_depth, y_depth<span class="o">]</span>, depth<span class="o">)</span>
            <span class="nv">t</span> <span class="o">=</span> TransformStamped<span class="o">()</span>
            <span class="c1"># Read message content and assign it to</span>
            <span class="c1"># corresponding tf variables</span>
            t.header.stamp <span class="o">=</span> self.get_clock<span class="o">()</span>.now<span class="o">()</span>.to_msg<span class="o">()</span>
            t.header.frame_id <span class="o">=</span> <span class="s1">&#39;camera_link&#39;</span>
            t.child_frame_id <span class="o">=</span> <span class="s1">&#39;user&#39;</span>

            <span class="c1"># Turtle only exists in 2D, thus we get x and y translation</span>
            <span class="c1"># coordinates from the message and set the z coordinate to 0</span>
            self.x_pos <span class="o">=</span> XYZ<span class="o">[</span><span class="m">2</span><span class="o">]</span>/1000.0
            self.y_pos <span class="o">=</span> -XYZ<span class="o">[</span><span class="m">0</span><span class="o">]</span>/1000.0
            t.transform.translation.x <span class="o">=</span> XYZ<span class="o">[</span><span class="m">2</span><span class="o">]</span>/1000.0 <span class="c1"># mm to m</span>
            t.transform.translation.y <span class="o">=</span> -XYZ<span class="o">[</span><span class="m">0</span><span class="o">]</span>/1000.0# mm to m
            t.transform.translation.z <span class="o">=</span> <span class="m">0</span>.0

            <span class="c1"># Send the transformation</span>
            self.tf_broadcaster.sendTransform<span class="o">(</span>t<span class="o">)</span>
except CvBridgeError as e:
    print<span class="o">(</span>e<span class="o">)</span>
    <span class="k">return</span>
</pre></div>
</div>
<p>imageDepthCallback method compute 3D coordinate relative to base frame of robot using pyrealsense2 libraly.It need depth image published by realsense camera</p>
</section>
<section id="test-environment">
<h3>Test environment<a class="headerlink" href="#test-environment" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Ubuntu 20.04</p></li>
<li><p>Ros foxy</p></li>
<li><p>Ubuntu kernel 5.11</p></li>
</ul>
</section>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h2>
<ul>
<li><p>ros package</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo apt-get install ros-foxy-cv-bridge
sudo apt-get install ros-foxy-realsense2-camera
sudo apt-get install ros-foxy-realsense2-camera-msgs
sudo apt-get install ros-foxy-realsense2-description
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>python package</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install mediapipe
pip install pyrealsense2
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>Open terminal in your GUI</p></li>
<li><p>In the same terminal run</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2 launch face_recognitions people_detection.launch.py
</pre></div>
</div>
<p>It will show camera feed tracking position of the body</p>
</div></blockquote>
</li>
<li><p>Open new terminal, run this command to start people follower</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2 service call /people_detection/enable std_srvs/srv/Empty
</pre></div>
</div>
<p>This command must be run when people_detection_scripts.py still tracking person otherwise it will not activate peole follower</p>
</div></blockquote>
</li>
<li><p>To end people follower type this command in terminal</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2 service call /people_detection/arrival std_srvs/srv/Empty
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
</section>
<section id="problem-and-future-plan">
<h2>Problem and future plan<a class="headerlink" href="#problem-and-future-plan" title="Permalink to this heading">¶</a></h2>
<p>tf listener is too slow for updating robot position and publish person coordinate relative to fixed frame map.We suggest to subscribe /tf topic for robot position because it is robot position relative to odom so we can compute person coordinate relative to fixed frame map</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">CACAO@HOME Robot</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">CACAO'S Robot application</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../fabrication/index.html">Fabrication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../navigation/index.html">Navigation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../manipulation/index.html">Manipulation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Perception</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../appendix/index.html">Appendix</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CACAO'S Software integration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../software_integration/index.html">Software Integration</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Perception</a><ul>
      <li>Previous: <a href="object_perception.html" title="previous chapter">Object Perception</a></li>
      <li>Next: <a href="speech_perception.html" title="next chapter">Speech Perception</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022-2022, Cacao Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/perception/docs/people_perception.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>
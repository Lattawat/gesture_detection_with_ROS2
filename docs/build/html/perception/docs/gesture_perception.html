
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Gesture Perception &#8212; CACAO@HOME Robot  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Appendix" href="../../appendix/index.html" />
    <link rel="prev" title="Speech Perception" href="speech_perception.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="gesture-perception">
<span id="id1"></span><h1>Gesture Perception<a class="headerlink" href="#gesture-perception" title="Permalink to this heading">¶</a></h1>
<h1 align="center">
  <div>
    <div style="position: relative; padding-bottom: 0%; overflow: hidden; max-width: 100%; height: auto;">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/4Jl3G3RK47c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>
  </div>
</h1><section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>The purpose of gesture perceptual application for sensing human behavior, living with the human the robot must know human behavior and interact with humans. The posture can specify many things whether emotion, desire, or action so gesture perception is necessary for living with humans. And the minor purpose is to command the robot as we say the posture can specify the desire. The basic way to command the robot is the posture and we can adjust the special command following the desire of the developer however it depends on the suitability to use in real life</p>
<section id="ros2-system-architecture">
<h3>ros2 system architecture<a class="headerlink" href="#ros2-system-architecture" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="../../_images/gesture_ros_arch.png"><img alt="ros2 system architecture" class="align-center" src="../../_images/gesture_ros_arch.png" style="width: 480px;" /></a>
<p>The gesture detection node consists of 2 services for calling from the behavior root node while the detect sequence is occurs</p>
</section>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>CV bridge LTS on ubuntu</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo apt-get install ros-<span class="o">(</span>ROS version name<span class="o">)</span>-cv-bridge
sudo apt-get install ros-<span class="o">(</span>ROS version name<span class="o">)</span>-vision-opencv
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Tensorflow==2.9.1</p>
<blockquote>
<div><p>see the installation reference on this link: <a class="reference external" href="https://www.tensorflow.org/install">https://www.tensorflow.org/install</a></p>
</div></blockquote>
</li>
</ol>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this heading">¶</a></h2>
<ul>
<li><p>First clone the repository from GitHub following this command</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/MBSE-2022-1/Software-Team.git -b gesture-perception
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Build the package (navigate to workspace directory before build)</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>colcon build --symlink-install
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Always navigate to workspace directory before build and symlink-install is necessary</p>
</div>
</div></blockquote>
</li>
<li><p>Run the package</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2 run gesture_detection gesturedetection.py
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Open realsense camera</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2 launch realsense2_camera rs_launch.py <span class="se">\</span>
            rgb_camera.profile:<span class="o">=</span>640x480x30 <span class="se">\</span>
            depth_module.profile:<span class="o">=</span>640x480x30 <span class="se">\</span>
            pointcloud.enable:<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Call service</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ros2 service call &lt;service name&gt; std_srvs/srv/Empty
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Permalink to this heading">¶</a></h2>
<a class="reference internal image-reference" href="../../_images/gesture_diagram.png"><img alt="gesture detection program structure" class="align-center" src="../../_images/gesture_diagram.png" style="width: 640px;" /></a>
<p>The gesture detection function consist of image subscriber, extract image feature, preprocessing and classifier model</p>
<ul>
<li><p><strong>Mediapipe</strong></p>
<p>the image feature is extracted by mediapipe library Mediapipe hand landmarks are composed of x, y, and z. x and y are normalized to [0.0, 1.0] by the image width and height respectively. z represents the landmark depth with the depth at the wrist being the origin, and the smaller the value the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x. The preprocessing function will set the wrist position as the origin point and then subtract the other 20 points from the origin point then normalize the position</p>
<p>See the reference API here: <a class="reference external" href="https://google.github.io/mediapipe/solutions/hands.html#python-solution-api">https://google.github.io/mediapipe/solutions/hands.html#python-solution-api</a></p>
</li>
<li><p><strong>preprocessing function</strong></p>
<dl class="rst directive">
<dt class="sig sig-object rst" id="directive-calc_landmark_list-self-landmarks">
<span class="sig-name descname"><span class="pre">..</span> <span class="pre">calc_landmark_list(self,</span> <span class="pre">landmarks)::</span></span><a class="headerlink" href="#directive-calc_landmark_list-self-landmarks" title="Permalink to this definition">¶</a></dt>
<dd><p>The hand landmarks from mediapipe are normalized [0.0, 1.0] this function will convert the normalized value to the picture position</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p>landmarks: normalize hand landmarks from the result of mediapipe</p>
</dd>
<dt class="field-even">Return<span class="colon">:</span></dt>
<dd class="field-even"><p>The same size of the input array respective to the image size with format [x, y, z]</p>
</dd>
</dl>
</dd></dl>

<dl class="rst directive">
<dt class="sig sig-object rst" id="directive-pre_process_landmark-self-landmark_list">
<span class="sig-name descname"><span class="pre">..</span> <span class="pre">pre_process_landmark(self,</span> <span class="pre">landmark_list)::</span></span><a class="headerlink" href="#directive-pre_process_landmark-self-landmark_list" title="Permalink to this definition">¶</a></dt>
<dd><p>This function using for preprocessing the hand landmark by subtracting all hand keypoint with the wrist position value and chaining the position x, y and z together</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p>landmark_list: list of hand landmarks respective to the image size in the format [x, y, z]</p>
</dd>
<dt class="field-even">Return<span class="colon">:</span></dt>
<dd class="field-even"><p>a dimension list of scale hand landmarks x follow by  y and z position</p>
</dd>
</dl>
</dd></dl>

<dl class="rst directive">
<dt class="sig sig-object rst" id="directive-calc_bounding_rect-self-landmarks">
<span class="sig-name descname"><span class="pre">..</span> <span class="pre">calc_bounding_rect(self,</span> <span class="pre">landmarks)::</span></span><a class="headerlink" href="#directive-calc_bounding_rect-self-landmarks" title="Permalink to this definition">¶</a></dt>
<dd><p>this function calculates the landmarks from mediapipe for the bounding box for debugging with the image</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p>landmarks: normalize hand landmarks from the result of mediapipe</p>
</dd>
<dt class="field-even">Return<span class="colon">:</span></dt>
<dd class="field-even"><p>[x, y, x + w, y + h] respectively to the image size</p>
</dd>
</dl>
</dd></dl>

</li>
<li><p><strong>Classifier model architecture</strong></p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/gesture_model.png"><img alt="gesture model architecture" class="align-center" src="../../_images/gesture_model.png" style="width: 640px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><p>Input: 42 length arrays</p>
<p>Output: hand class [‘Open’, ‘Start_cmd’, ‘Pointer’, ‘Close’, ‘OK’]</p>
</div></blockquote>
</section>
<section id="problem-and-future-plan">
<h2>Problem and future plan<a class="headerlink" href="#problem-and-future-plan" title="Permalink to this heading">¶</a></h2>
<p>Gesture perception tasks on <a class="reference external" href="mailto:robocup&#37;&#52;&#48;home">robocup<span>&#64;</span>home</a></p>
<ul>
<li><p>Task 5.1 carry my luggage</p>
<blockquote>
<div><ul class="simple">
<li><p>Detect start command</p></li>
<li><p>Detect pointing position</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Task 5.3 Farewell</p>
<blockquote>
<div><ul class="simple">
<li><p>Detect tired user</p></li>
<li><p>Detect calling and signal to leave</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Task 6.4 Hand Me That</p>
<blockquote>
<div><ul class="simple">
<li><p>Detect pointing position</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Task 6.6 Restaurant</p>
<blockquote>
<div><ul class="simple">
<li><p>Detect calling and waving</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Task 6.7 Smoothie Chef</p>
<blockquote>
<div><ul class="simple">
<li><p>Learning how to prepare the smoothie and then follow</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Task 6.8 Stickler for the Rules</p>
<blockquote>
<div><ul class="simple">
<li><p>Identify rule breaker</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This semester we are only planning for task 5.1</p>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">CACAO@HOME Robot</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">CACAO'S Robot application</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../fabrication/index.html">Fabrication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../navigation/index.html">Navigation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../manipulation/index.html">Manipulation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Perception</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../appendix/index.html">Appendix</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CACAO'S Software integration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../software_integration/index.html">Software Integration</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Perception</a><ul>
      <li>Previous: <a href="speech_perception.html" title="previous chapter">Speech Perception</a></li>
      <li>Next: <a href="../../appendix/index.html" title="next chapter">Appendix</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022-2022, Cacao Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/perception/docs/gesture_perception.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>